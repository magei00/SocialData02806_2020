{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Questions to text and lectures.\n",
    "\n",
    "### A) Segal and Heer paper\n",
    "\n",
    "* **What is the Oxford English Dictionary's defintion of a narrative?**\n",
    "\n",
    "“an account of a series of events, facts, etc., given in order and with the establishing of connections between them.”\n",
    "\n",
    "\n",
    "* **What is your favorite visualization among the examples in section 3? Explain why in a few words.**\n",
    "\n",
    "Figure 2 comparing budget forecasts with reality is a great visualization because even though its simple, it elegantly conveys the narrative through usage of the martini glass structure. The visualiztion shows a good amount of information and the layout is consitent ensuring that reader doesn't get disoriented and is able to understand the story. Finally the visualization uses a tacit tutorial to ensure that the reader knows which interactions are possible.\n",
    "\n",
    "* **What's the point of Figure 7?**\n",
    "\n",
    "Figure 7 shows an analysis of tactics used for visual narrative as well as narrative structures used acros mutliple genres of visualization. The figure highlights trends in specific genres, it showcases which strategies are commonly used as well as which are under utilized. \n",
    "\n",
    "* **Use Figure 7 to find the most common design choice within each category for the Visual narrative and Narrative structure**\n",
    "\n",
    "The most common choices are shown in the following table:\n",
    "\n",
    "| Visual Structuring        | Highlighting        | Transition Guidance | Ordering           | Interactivity                  | Messaging           |\n",
    "|---------------------------|---------------------|---------------------|--------------------|--------------------------------|---------------------|\n",
    "| Consitent Visual Platform | Feature Distinction | Object Continuity   | User Directed Path | Filtering / Selection / Search | Caption / Headlines |\n",
    "\n",
    "\n",
    "* Visual Narrative\n",
    "\n",
    "    The most common choice for visual structuring is a consistent platform. Feature distinction is by far the most popular highlighting strategy. Object continuety is the most used transition gudiance strategy, closely followed by animated transition s as well as familiar objects. \n",
    "\n",
    "\n",
    "* Narrative Structure\n",
    "\n",
    "    User directed path is by far the most used Ordering strategy. Filtering/selection/search is almost always used for interactivity. Captions/Headlines are used very frequently closly followed by annotations.\n",
    "\n",
    "\n",
    "\n",
    "* **Check out Figure 8 and section 4.3. What is your favorite genre of narrative visualization? Why? What is your least favorite genre? Why?**\n",
    "\n",
    "Slide show is great for narrative visualization because it can utilize a vast amount of strategies in regards of both visual narrative as well as narrative structure. It can utilize interactivity well, letting the user explore at their own pace and to the desired extend, while making sure the narrative is pressentend in the intended order from start to end.\n",
    "\n",
    "Magazine style is the most limiting medium in regards of which narrative strategies can be utiliez and is therfore the least interresting.\n",
    "\n",
    "### B) Explanatory data visualization\n",
    "\n",
    "* **What are the three key elements to keep in mind when you design an explanatory visualization?**\n",
    "\n",
    "    * The first key point is to ask yourself, what results you want to communicate to the audience. \n",
    "    * The second keypoint is to allow exploration by utilizing user interactions in order to further engage the audience.\n",
    "    * The third keypoint is you should know the audience and design the visualization accordingly.\n",
    "\n",
    "* **In the video I talk about (1) overview first, (2) zoom and filter, (3) details on demand.**\n",
    "\n",
    "    * **Go online and find a visualization that follows these principles (don't use one from the video).**\n",
    "    \n",
    "    <img src=\"https://magei.dk/wp-content/uploads/2020/03/FacebookVisualization.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "   \n",
    "   The visualization is a Slide show showing how Facebook compares to other tech IPO's. It can be viewed in the following link:\n",
    "    https://archive.nytimes.com/www.nytimes.com/interactive/2012/05/17/business/dealbook/how-the-facebook-offering-compares.html?ref=technology\n",
    "    \n",
    "\n",
    "* **Explain how it achieves (1)-(3). It might be useful to use screenshots to illustrate your explanation.**\n",
    "    \n",
    "     **(1)** On the first slide an overview is given of previous IPO's to set the scene. **(2)** On the following slides zoom is used to change the users perspective. **(3)** At all times its possible to hower over the companies to get details on demand. It is also possible to search for a specific company.\n",
    "    \n",
    "    \n",
    "* **Explain in your own words: How is explanatory data analysis different from exploratory data analysis?**\n",
    "\n",
    "Explanatory data analysis is used to communicate results from a dataset to an audience, which means that the way the results are visualized is of utmost importance. Explanatory data analysis is all about creating a narrative through varius visualization strategies, that is engaging for the user. \n",
    "On the other hand exploratory data analysis is usefull for figuring out what narrative (if any) the dataset is able to tell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Random forest and weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import datetime as dt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting without weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "police_data = pd.read_csv(\"../../police_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "police_data['Time'] = pd.to_datetime(police_data['Time'],format=\"%H:%M\")\n",
    "police_data['Date'] = pd.to_datetime(police_data['Date'],format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "police_data = police_data[(police_data['Date'] > '2012-10-01') & (police_data['Date'] < '2017-10-27')]\n",
    "\n",
    "new_pd = police_data[['Category', 'Date','Time','DayOfWeek','PdDistrict']]\n",
    "new_pd['Hour'] = new_pd['Time'].dt.hour\n",
    "new_pd['Month'] = new_pd['Date'].dt.month\n",
    "new_pd['Hour_of_week'] = new_pd['Date'].dt.dayofweek * 24 + (new_pd['Hour'] + 1)\n",
    "new_pd['DayOfWeek'] = new_pd['Date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories to use, framing the data\n",
    "\n",
    "We define two categories, burglary and fraud. These are chosen because we hypothesize that burglaries are very impacted by weather, while fraud is not, and we want to see if weather have any effect on these crimes.\n",
    "\n",
    "We use balanced datasets, with 10000 samples in each. We chose balanced simply because otherwise we might well have way too many burglary incidents compared to fraud, or the other way around, which would skew our model and ultimately impact later predictions. \n",
    "\n",
    "We then combine the datasets to one dataframe, which we then again split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "burglaries = new_pd[(new_pd['Category'] == 'BURGLARY')]\n",
    "fraud = new_pd[(new_pd['Category'] == 'FRAUD')]\n",
    "\n",
    "balanced_burglaries = burglaries.sample(10000, random_state=1)\n",
    "balanced_fraud = fraud.sample(10000, random_state=1)\n",
    "combined_set = pd.concat([balanced_burglaries,balanced_fraud],axis=0)\n",
    "combined_set['PdDistrict'] = pd.factorize(combined_set['PdDistrict'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(combined_set, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the time and date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=['Category','Time','Date'])\n",
    "y_train = train['Category']\n",
    "\n",
    "X_test = test.drop(columns=['Category','Time','Date'])\n",
    "y_test = test['Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a small function for testing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the models\n",
    "\n",
    "We define two distinct random forest classifiers, base_rfc, which represents a default instance of the random forest classfier that SKLearn provides, and later on we define **improved_rfc**. On improved_rfc we perform a Randomized Search for best hyperparameters, so as to find the 'best' parameters for our model, without having the manually adjust these. **Beware that testing for parameters takes a while**, so we've included the results of the testing below in the dict 'dbest_params'. No need to actually perform the testing on the readers part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.574\n"
     ]
    }
   ],
   "source": [
    "base_rfc = RandomForestClassifier()\n",
    "test_model(base_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the parameters and different values to test with. These are chosen with speed in mind, as testing for parameters takes a while. We have tried to cover most sensible values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [1,5,20,50,100,200,500]\n",
    "min_samples_leaf = [1,5,20,50,100,200,500]\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 5, 20, 50, 100,\n",
       "                                                             200, 500],\n",
       "                                        'min_samples_split': [1, 5, 20, 50, 100,\n",
       "                                                              200, 500],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rfc_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'max_depth': 10, 'max_features': 'auto', 'n_estimators': 2000, 'min_samples_split': 100, 'min_samples_leaf': 20, 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbest_params = {'max_depth': 10, 'max_features': 'auto', 'n_estimators': 1000, 'min_samples_split': 100, 'min_samples_leaf': 20, 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_rfc = RandomForestClassifier()\n",
    "improved_rfc.set_params(**best_params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62325\n"
     ]
    }
   ],
   "source": [
    "test_model(improved_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did testing for hyperparameters help?\n",
    "\n",
    "Well yes, it did. base_rfc has accuracy of about 0.59 - 0.60, while our improved model is around 0.64. \n",
    "A small improvement but anything counts!\n",
    "\n",
    "Now we are interesting in testing for features, or more accurately, testing for how many features we ideally use.\n",
    "For this we use SKLearns Recursive feature elimination with cross-validation (or, RFECV). This way we contiously fit our model and remove a feature at each step, and cross validate along the way. The end result is a model fitted with the best amount of features, that also avoids overfitting due to cross-validation.\n",
    "The impact on accuracy, however, is barely noticable and remains about 0.64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv_pre = RFECV(estimator = improved_rfc, step=1, cv=StratifiedKFold(2),scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=StratifiedKFold(n_splits=2, random_state=None, shuffle=False),\n",
       "      estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                       class_weight=None, criterion='gini',\n",
       "                                       max_depth=10, max_features='auto',\n",
       "                                       max_leaf_nodes=None, max_samples=None,\n",
       "                                       min_impurity_decrease=0.0,\n",
       "                                       min_impurity_split=None,\n",
       "                                       min_samples_leaf=20,\n",
       "                                       min_samples_split=100,\n",
       "                                       min_weight_fraction_leaf=0.0,\n",
       "                                       n_estimators=2000, n_jobs=None,\n",
       "                                       oob_score=False, random_state=None,\n",
       "                                       verbose=0, warm_start=False),\n",
       "      min_features_to_select=1, n_jobs=None, scoring='accuracy', step=1,\n",
       "      verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv_pre.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d748828d2198>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrfecv_pre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of features selected\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cross validation score (nb of correct classifications)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfecv_pre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrfecv_pre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "rfecv_pre.score(X_test, y_test)\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv_pre.grid_scores_) + 1), rfecv_pre.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv_pre.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv(\"../weather_data.csv\")\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'],infer_datetime_format=\"%Y/%m/%d\",format=\"%d/%m/%Y\")\n",
    "weather_data.dropna()\n",
    "weather_data['date'] = weather_data['date'].dt.tz_localize(None)\n",
    "weather_data['hour'] = weather_data['date'].dt.hour\n",
    "weather_data['date'] = weather_data['date'].dt.date\n",
    "\n",
    "new_pd['Date'] = new_pd['Date'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the two datasets and repeat the steps in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.merge(new_pd,weather_data,how='left',left_on=['Date','Hour'], right_on=['date','hour'])\n",
    "new_df = new_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burglaries = new_df[(new_df['Category'] == 'BURGLARY')]\n",
    "fraud = new_df[(new_df['Category'] == 'FRAUD')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features 'weather' and 'PdDistrict' is again factorized to allow the classfier to utilize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_burglaries = burglaries.sample(10000, random_state=1)\n",
    "balanced_fraud = fraud.sample(10000, random_state=1)\n",
    "combined_set = pd.concat([balanced_burglaries,balanced_fraud],axis=0)\n",
    "combined_set['PdDistrict'] = pd.factorize(combined_set['PdDistrict'])[0]\n",
    "combined_set['weather'] = pd.factorize(combined_set['weather'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weather, test_weather = train_test_split(combined_set, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weather = train_weather.drop(columns=['Category','Time','Date','date'])\n",
    "y_train_weather = train_weather['Category']\n",
    "\n",
    "X_test_weather = test_weather.drop(columns=['Category','Time','Date','date'])\n",
    "y_test_weather = test_weather['Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What features to use?\n",
    "\n",
    "Instead of removing features we think may or may not work in regards to the weather, we once again turn to RFECV. \n",
    "Adding weather data means that we now have 11 features total, so performing the RFECV now takes a little longer. Again, for the readers sake, we've included the results below so no need to fit the RFECV.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv = RFECV(estimator = improved_rfc, step=1, cv=StratifiedKFold(2),scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.fit(X_train_weather,y_train_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a small plot of how our accuracy changes according to features used. 5 is optimal and adding more lowers the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.score(X_test_weather,y_test_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_rfc.fit(X_train_weather, y_train_weather)\n",
    "improved_rfc.score(X_test_weather, y_test_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rfc.fit(X_train_weather, y_train_weather)\n",
    "base_rfc.score(X_test_weather, y_test_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final results\n",
    "\n",
    "To avoid having the user perform RFECV, here are the final results:\n",
    "\n",
    "## Crime-data without weather:\n",
    "\n",
    "Naive Random Forest Classifer (without finding best hyperparameters or performing RFECV:\n",
    "~ 0.59\n",
    "\n",
    "w. best hyperparameters:\n",
    "~ 0.64\n",
    "\n",
    "w. best hyperparameters and best features:\n",
    "~ 0.64\n",
    "\n",
    "## Crime-data with weather:\n",
    "\n",
    "Naive Random Forest Classifer (without finding best hyperparameters or performing RFECV:\n",
    "~ 0.6\n",
    "\n",
    "w. best hyperparameters:\n",
    "~ 0.62\n",
    "\n",
    "w. best hyperparameters and best features:\n",
    "~ 0.63\n",
    "\n",
    "As it turns out, adding weather data to our original crime dataset did not improve accuracy. We can now make a somewhat educated guess and say that neither burglary nor fraud depends severely on the current weather situation.\n",
    "\n",
    "But, we can predict what type of crime will take base on some relevant features, with about 64% accuracy. This is by no means perfect, but it's better than purely guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = rfecv.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test_weather.columns.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What were the most significant features?\n",
    "\n",
    "Here's the ranking, with lower being better. DayOfWeek scores worst and has little to no positive impact. Same goes for weather, wind speed and month. Temperature turned out to be relatively important, so did hour, police district and , surprisingly, wind direction.\n",
    "\n",
    "* Ranking of DayOfWeek : 8\n",
    "* Ranking of PdDistrict : 1\n",
    "* Ranking of Hour : 1\n",
    "* Ranking of Month : 5\n",
    "* Ranking of Hour_of_week : 1\n",
    "* Ranking of temperature : 1\n",
    "* Ranking of humidity : 3\n",
    "* Ranking of weather : 6\n",
    "* Ranking of wind_speed : 7\n",
    "* Ranking of wind_direction : 2\n",
    "* Ranking of pressure : 4\n",
    "* Ranking of hour : 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ranks)):\n",
    "    print(\"Ranking of \" + features[i] + \" : \" + str(ranks[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we need to do is import the data and convert it to the correct format.\n",
    "\n",
    "We choose to only look at the focuscrimes.<br>\n",
    "Then we want the date data to be easily accesible so we extract all the relevant information and put it in seperate collumns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "focuscrimes = set(['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'])\n",
    "police_data_all = pd.read_csv('../../police_data.csv')\n",
    "\n",
    "police_data = police_data_all.where(police_data_all.Category.isin(focuscrimes))\n",
    "\n",
    "police_data['Date'] = pd.to_datetime(police_data['Date'], format=\"%m/%d/%Y\")\n",
    "police_data['Time'] = pd.to_datetime(police_data['Time'], format=\"%H:%M\")\n",
    "police_data['Year'] = police_data['Date'].dt.year\n",
    "police_data['Month'] = police_data['Date'].dt.month\n",
    "police_data['Hour'] = police_data['Time'].dt.hour\n",
    "police_data['Hour_of_week'] = police_data['Date'].dt.dayofweek * 24 + (police_data['Hour'] + 1)\n",
    "police_data['Day'] = police_data['Date'].dt.day\n",
    "police_data['Minute'] = police_data['Time'].dt.minute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the data imported we normalize it so it better compares between different crimetypes.\n",
    "\n",
    "We do this by splitting every category up into 24 chunks of 1 hour, and the dividing each chunk by the total amount of that crime.\n",
    "\n",
    "This results in a value between 0 and 1 that describes the percentage of a given crime in that time interval.<br>\n",
    "The results can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crime_hour_norm_df = pd.DataFrame()\n",
    "crime_hour_norm_df.insert(0, 'Hour', np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23])) \n",
    "for i, category in enumerate(focuscrimes):\n",
    "    df = police_data[police_data.Category == category]\n",
    "    df_hour = df['Category'].groupby(df['Hour']).count()\n",
    "    \n",
    "    #normalize\n",
    "    df_hour= df_hour/df['Category'].count()\n",
    "    \n",
    "    #insert normalized data into dataframe\n",
    "    crime_hour_norm_df.insert(i+1, category, df_hour) \n",
    "    \n",
    "\n",
    "crime_hour_norm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the data ready we just need to displat it in the Bokeh plot.\n",
    "\n",
    "This is mostly just fiddling with a lot of settings, <br>\n",
    "in colors we store all the colors for the bars.<br>\n",
    "In items we store the different crimes to create a legend later, notice how we link it to the bars in line 23 items.append((i, [bar[i]])) <br>\n",
    "\n",
    "Most of the settings make sense on their own, but some interesting ones that we played with was: <br>\n",
    "* visible, we decided to hide all the graphs except for one to begin with, since you usually only want to compare two at a time, and turning everything off is tedious.\n",
    "* fill_alpha, this is the transparency of the bars and we needed to set it to something lower that 1 so that the bars would read through eachother.\n",
    "* toolbar_location, we decided to hide the main toolbar since it didn't add much functionality and just cluttered up the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_file, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models import FactorRange\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import Legend\n",
    "\n",
    "output_notebook() # for outputting to notebook\n",
    "source = ColumnDataSource(crime_hour_norm_df) # data importing\n",
    "colors = [\"#a83232\", \"#a86932\", \"#a8a232\", \"#7da832\", \"#32a83c\", \"#32a87f\", \"#3283a8\",\"#324aa8\", \"#5d32a8\", \"#9432a8\", \"#a83273\", \"#a83248\", \"#b59399\", \"#2e292a\"]\n",
    "\n",
    "hours = []\n",
    "for i in range(24):\n",
    "    hours.append(str(i))\n",
    "\n",
    "p = figure(x_range = FactorRange(factors=hours), plot_height=400, plot_width=900, title=\"Normalized crime rate over a day\", \n",
    "           toolbar_location=None)\n",
    "items=[]\n",
    "bar ={} # to store vbars\n",
    "\n",
    "for indx,i in enumerate(focuscrimes):\n",
    "    \n",
    "    bar[i] = p.vbar(x='Hour',  top=i, source= source, visible = False, width= 0.8, color =colors[indx], fill_alpha =0.6) \n",
    "    items.append((i, [bar[i]]))\n",
    "    \n",
    "#p.legend.click_policy=\"hide\" ### assigns the click policy (you can try to use ''hide'\n",
    "#p.legend.location = 'top_left'\n",
    "\n",
    "legend = Legend(items=items, location=(0,0))\n",
    "legend.click_policy=\"hide\"\n",
    "\n",
    "p.add_layout(legend, 'left')\n",
    "p.xaxis.axis_label = \"Hour of day\"\n",
    "p.yaxis.axis_label = \"Percentage of crime\"\n",
    "\n",
    "bar['TRESPASS'].visible = True #we start with weapons laws just to display something\n",
    "\n",
    "show(p) #displays your plot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heres a gif of it!\n",
    "<img src=\"https://magei.dk/wp-content/uploads/2020/03/LevelEditor.gif\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
